{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Note on Econometrics</h1></center>\n",
    "<center>Haixi Li</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "- [Finite Sample Property](#Finite-Sample_Property)\n",
    "- [Large Sample Theory](#Large-Sample-Theory)\n",
    "- [Time Series Analysis](#Time-Series-Analysis)\n",
    "- [Inverting Lag Polynomials](#Inverting-Lag-Polynomials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assumption for Classic Linear Regression \n",
    "The classic linear regression model starts with a few assumptions\n",
    "#### Assumption 1.1 Linearity\n",
    "\\begin{equation}\n",
    "y_{t} = X'\\beta + \\epsilon_{t}\n",
    "\\end{equation}\n",
    "#### Assumption 1.2 Strict Exogeneity\n",
    "\\begin{equation}\n",
    "E(\\epsilon_{i}\\mid X) = 0 \\text{ for } i=1,2,...,n. \n",
    "\\end{equation}\n",
    "#### Assumption 1.3 No Multicollinerity\n",
    "The rank of the $n\\times K$ data matrix, $X$ is $K$ with probability 1. \n",
    "#### Assumption 1.4 Spherical Error Variance\n",
    "homoskedasticity\n",
    "\\begin{equation}\n",
    "E(\\epsilon_{i}^{2}\\mid X)=\\sigma^{2} \\text{ for } i=1,2,...,n\n",
    "\\end{equation}\n",
    "no correlation betwee observations\n",
    "\\begin{equation}\n",
    "E(\\epsilon_{i}\\epsilon_{j}) = 0 \\, (i,j=1,2,...,n. i \\neq j)\n",
    "\\end{equation}\n",
    "#### Assumption 1.5 Normality of the Error Term\n",
    "The distribution of $\\epsilon$ conditional on $X$ is jointly normal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assumption 1.5 together with 1.2 and 1.4, we have\n",
    "\\begin{equation}\n",
    "\\epsilon \\mid X \\sim \\mathcal{N}(0,\\sigma^{2}I_{n})\n",
    "\\end{equation}\n",
    "Given the 1.2 exogeneity assumption, we can conclude that the distribution of $\\epsilon$ is independent of $X$. Therefore, in particular, the marginal or unconditional distribution of $epsilon$ is\n",
    "\\begin{equation}\n",
    "\\epsilon \\sim \\mathcal{N}(0,\\sigma^{2}I_{n})\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finite-Sample Properties of OLS\n",
    "#### Unbiasedness \n",
    "under the assumption 1.1-1.3 $E(\\hat{\\beta}\\mid X)=\\beta$\n",
    "\\begin{proof}\n",
    "\n",
    "$E(\\hat{\\beta}-\\beta\\mid X)=E((X'X)^{-1}X'y-\\beta\\mid X)$\n",
    "\n",
    "$E((X'X)^{-1}X'(X\\beta+\\epsilon)-\\beta \\mid X)$\n",
    "\n",
    "$E((X'X)^{-1}X'\\epsilon \\mid X)=(X'X)^{-1}X'E(\\epsilon\\mid X)=0$\n",
    "\\end{proof}\n",
    "#### Expression for the Variance\n",
    "under the assumption 1.1-1.4 $var(\\hat{\\beta}\\mid X)=\\sigma^{2}(X'X)^{-1}$.\n",
    "\\begin{proof}\n",
    "\\end{proof}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Errors vs Residuals\n",
    "Errors pertain to the true data generating process (DGP), whereas residuals are what is left over after having estimated your model. In truth, assumptions like normality, homoscedasticity, and independence apply to the errors of the DGP, not your model's residuals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finite Sample Property\n",
    "The finite sample property rely on distributional assumptions to derive the distribution for estimators. Large sample theory, on the contrast, due to Central Limit Theorom, can relax distributional assumptions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Large Sample Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series Analysis\n",
    "A white noise process $\\varepsilon_{t}$ is a zero mean and covariance stationary process with no serial correlation:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "E(\\varepsilon_{t}) &=& 0\\\\\n",
    "E(\\varepsilon_{t}^{2}) &=&\\sigma^{2}\\\\\n",
    "E(\\varepsilon_{t}\\varepsilon_{t-j}) &=& 0 \\, \\text{for} \\, j \\neq 0.\n",
    "\\end{eqnarray}\n",
    "\n",
    "A very import class of covariance stationary processes, called linear processes, can be created by taking a moving average of a white noise process.\n",
    "We say a value is random, that means the variable has a probability distribution associated with $t$.\n",
    "The reason for this is we have not other \n",
    "\n",
    "\\begin{equation}\n",
    "\\alpha + \\beta =\\sum x_{i} \\label{eq2}\n",
    "\\end{equation}\n",
    "\n",
    "We will refer this equation as $\\ref{eq2}$ as the second equation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverting Lag Polynomials\n",
    "The $pth$ degree of lag operator \n",
    "\n",
    "\\begin{equation}\n",
    "\\phi(L) = 1 - \\phi_{1}L -\\phi_{2}L^{2}-...-\\phi_{p}L^{P} \\label{eq1}\n",
    "\\end{equation}\n",
    "\n",
    "is invertable if the p-th degree polynomial equation in $z$\n",
    "\n",
    "\\begin{equation}\n",
    "\\phi(z) = 0 \\, \\text{where} \\, \\phi(z)\\equiv 1-\\phi_{1}z-\\phi_{2}z^{2}-...-\\phi_{p}z^{p}\n",
    "\\end{equation}\n",
    "\n",
    "are greater than 1 in absolute value.\n",
    "we are referring $\\ref{eq1}$\n",
    "### Large Sample Theory\n",
    "#### Convergence in Probability\n",
    "A sequence of random scalar variables $\\{ z_{n}\\}$ converges in probability to a constant $\\alpha$, if for any $\\varepsilon > 0$,\n",
    "\\begin{equation}\n",
    "\\lim_{n \\rightarrow \\infty} \\text{Prob}(\\mid z_{n} - \\alpha\\mid > \\varepsilon) = 0.\n",
    "\\end{equation}\n",
    "The constant $\\alpha$ is called the probability limit of $z_{n}$. Each $z_{n}$ is a random variable, hence has an distribution associated with it. As $n$ goes to infinity, the distribution will collapses around $\\alpha$. The probability mass will center around $\\alpha$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series\n",
    "### Basic Concept\n",
    "##### Stochastic Process\n",
    "A stochastic process is a sequence of random variables. $\\{z_{t}\\}$ is a sequence of random variables, then its realization is a sequence of real number.\n",
    "\n",
    "The fundamental problem of time series analysis is we can observe the realization of the process only once. If the all the realization of the process comes from the same distribution, then we can still learn from the time series about the distribution which is the realization of the same distribution at different time.\n",
    "\n",
    "Furthermore, if the process is not too persistent, that each realization represent some information others don't have. This property is called ergodicity.\n",
    "##### Ststionarity\n",
    "Strick stationarity means the probability distribution does not change across the stochastic process.\n",
    "Weak stationarity suggest $E(z_{t}) \\, \\text{does not depend on t}$\n",
    "\n",
    "\\begin{equation}\n",
    "cov(z_{t},z_{t-j}) \\, \\text{exists, in finite, and depends only on j but not on t}\n",
    "\\end{equation}\n",
    "##### Ergodic\n",
    "Heuristically, a stationary process is ergodic if it is asymptotically independent, that is, if any two random variables positioned far apart in the sequence are almost independently distributed. Ergodic stationary will be an integral ingredient in developing large-sample theory because of the following property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "author": "mes",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": false,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
